{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Actors Wiki data using BeautifulSoup \n",
    "#### Dependency - installation of BeautifulSoup Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\vidhya\\anaconda3\\envs\\cv-projv3\\lib\\site-packages (19.1)\n",
      "Collecting install\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Could not find a version that satisfies the requirement install (from versions: none)\n",
      "ERROR: No matching distribution found for install\n"
     ]
    }
   ],
   "source": [
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki list of American actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "BASE_URL = 'https://en.wikipedia.org'\n",
    "WIKI_ACTORS_URL = BASE_URL+ '/wiki/Category:American_male_film_actors'\n",
    "total_added = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request(\"GET\", url)\n",
    "    return BeautifulSoup(r.data, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing -  citation removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove citations and punctuations\n",
    "def preprocessingText(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokens = []\n",
    "    temp = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # Removing punctuations except '<.>/<?>/<!>'\n",
    "        punctuations = '\"#$%&\\'()*+,-/:;<=>@\\\\^_`{|}~'\n",
    "        words = map(lambda x: x.translate(str.maketrans('', '', punctuations)), words)\n",
    "        \n",
    "        # Remove empty strings\n",
    "        words = filter(lambda x: len(x) > 0, words)\n",
    "      \n",
    "        tokens = tokens + list(words)\n",
    "        temp = ' '.join(word for word in tokens)\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing each actor webpage and writing into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the given content into text file with name <title>.txt\n",
    "def write_text_into_file(title, data):\n",
    "    filename = \".\\ActorsDataset\\\\\" + title + \".txt\"\n",
    "    f = open(filename, 'w+', encoding=\"utf-8\")\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "    print(\"Text file \" + filename + \" created\")\n",
    "        \n",
    "# Parse each actor webpage content\n",
    "def parse_actor_content(link):\n",
    "    soup = get_soup(link)\n",
    "    results = soup.find_all(\"div\", {\"class\": \"mw-parser-output\"})[0]\n",
    "    no_of_paragraphs = 0\n",
    "    paragraphs = results.find_all('p')\n",
    "    data = \"\"\n",
    "    for para in paragraphs:\n",
    "        if para.id != \"mw-empty-elt\":\n",
    "            data += para.text.strip() +\"\\n\"\n",
    "            no_of_paragraphs += 1\n",
    "        if no_of_paragraphs == 3:\n",
    "            break\n",
    "            \n",
    "#   extracting sentences from the paragraph\n",
    "    data = \".\".join(data.split(\".\")[:2])\n",
    "    modified_data = preprocessingText(data+\".\") \n",
    "    return modified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing all actors' content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through every group\n",
    "def parse_all_actors_from_wiki(url):\n",
    "    soup = get_soup(url)\n",
    "    results = soup.find_all(\"div\", {\"class\":\"mw-category-group\"})\n",
    "    no_of_actors = 0\n",
    "    for res in results:\n",
    "        # iterator through every actor or <li> element\n",
    "        li_list = res.find_next('ul').find_all('li')\n",
    "        for li in li_list:\n",
    "            name = li.a.text.strip()\n",
    "            link = li.a['href'].strip()\n",
    "            data = parse_actor_content(BASE_URL+link)\n",
    "            write_text_into_file(name, data)\n",
    "            no_of_actors += 1\n",
    "            if no_of_actors == 25:\n",
    "                break\n",
    "    print(no_of_actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parse_all_actors_from_wiki(WIKI_ACTORS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file .\\ActorsDataset\\50 Cent.txt created\n",
      "Text file .\\ActorsDataset\\Lee Aaker.txt created\n",
      "Text file .\\ActorsDataset\\Willie Aames.txt created\n",
      "Text file .\\ActorsDataset\\Quinton Aaron.txt created\n",
      "Text file .\\ActorsDataset\\Victor Aaron.txt created\n",
      "Text file .\\ActorsDataset\\Abbott and Costello.txt created\n",
      "Text file .\\ActorsDataset\\Bruce Abbott.txt created\n",
      "Text file .\\ActorsDataset\\Bud Abbott.txt created\n",
      "Text file .\\ActorsDataset\\Christopher Abbott.txt created\n",
      "Text file .\\ActorsDataset\\Philip Abbott.txt created\n",
      "Text file .\\ActorsDataset\\Richard Abbott (actor).txt created\n",
      "Text file .\\ActorsDataset\\Jake Abel.txt created\n",
      "Text file .\\ActorsDataset\\Walter Abel.txt created\n",
      "Text file .\\ActorsDataset\\Zachary Abel.txt created\n",
      "Text file .\\ActorsDataset\\F. Murray Abraham.txt created\n",
      "Text file .\\ActorsDataset\\Jon Abrahams.txt created\n",
      "Text file .\\ActorsDataset\\Omid Abtahi.txt created\n",
      "Text file .\\ActorsDataset\\Yousef Abu-Taleb.txt created\n",
      "Text file .\\ActorsDataset\\Kirk Acevedo.txt created\n",
      "Text file .\\ActorsDataset\\Jensen Ackles.txt created\n",
      "Text file .\\ActorsDataset\\Rodolfo Acosta.txt created\n",
      "Text file .\\ActorsDataset\\Jay Acovone.txt created\n",
      "Text file .\\ActorsDataset\\Eddie Acuff.txt created\n",
      "Text file .\\ActorsDataset\\Jason Acu√±a.txt created\n",
      "Text file .\\ActorsDataset\\Ad-Rock.txt created\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
